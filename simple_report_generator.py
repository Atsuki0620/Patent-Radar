#!/usr/bin/env python3
"""
Simplified report generator for testing evaluation
"""

import json
import pandas as pd
from pathlib import Path
from datetime import datetime

def calculate_binary_metrics(tp, fp, tn, fn):
    """Calculate basic binary classification metrics"""
    accuracy = (tp + tn) / (tp + fp + tn + fn) if (tp + fp + tn + fn) > 0 else 0
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
    
    return {
        'accuracy': accuracy,
        'precision': precision, 
        'recall': recall,
        'f1': f1,
        'specificity': specificity
    }

def load_and_analyze():
    """Load data and perform analysis"""
    
    # Load results
    results_path = Path("archive/outputs/testing_results.jsonl")
    labels_path = Path("testing/data/labels.jsonl")
    
    predictions = []
    with open(results_path, 'r', encoding='utf-8') as f:
        for line in f:
            predictions.append(json.loads(line.strip()))
    
    labels = {}
    with open(labels_path, 'r', encoding='utf-8') as f:
        for line in f:
            label_data = json.loads(line.strip())
            labels[label_data['publication_number']] = label_data
    
    # Analyze results
    total_patents = len(predictions)
    hit_predictions = [p for p in predictions if p['decision'] == 'hit']
    miss_predictions = [p for p in predictions if p['decision'] == 'miss']
    
    # Calculate confusion matrix (excluding borderline from binary evaluation)
    tp = fp = tn = fn = 0
    borderline_count = 0
    
    gold_distribution = {'hit': 0, 'miss': 0, 'borderline': 0}
    pred_distribution = {'hit': 0, 'miss': 0, 'borderline': 0}
    
    for pred in predictions:
        pub_num = pred['publication_number']
        gold_label = labels.get(pub_num, {}).get('gold_label', 'miss')
        pred_decision = pred['decision']
        
        gold_distribution[gold_label] += 1
        pred_distribution[pred_decision] += 1
        
        if gold_label == 'borderline':
            borderline_count += 1
            continue
            
        # Binary classification metrics (hit vs miss only)
        if gold_label == 'hit' and pred_decision == 'hit':
            tp += 1
        elif gold_label == 'miss' and pred_decision == 'hit':
            fp += 1
        elif gold_label == 'miss' and pred_decision == 'miss':
            tn += 1
        elif gold_label == 'hit' and pred_decision == 'miss':
            fn += 1
    
    binary_total = tp + fp + tn + fn
    metrics = calculate_binary_metrics(tp, fp, tn, fn)
    
    # Confidence analysis
    hit_confidences = [p['confidence'] for p in predictions if p['decision'] == 'hit']
    miss_confidences = [p['confidence'] for p in predictions if p['decision'] == 'miss']
    
    avg_hit_conf = sum(hit_confidences) / len(hit_confidences) if hit_confidences else 0
    avg_miss_conf = sum(miss_confidences) / len(miss_confidences) if miss_confidences else 0
    
    # Ranking analysis - how well do hits rank?
    predictions_sorted = sorted(predictions, key=lambda x: (-x['confidence'], x['pub_number']))
    
    hit_ranks = []
    for i, pred in enumerate(predictions_sorted):
        pub_num = pred['publication_number']
        gold_label = labels.get(pub_num, {}).get('gold_label', 'miss')
        if gold_label == 'hit':
            hit_ranks.append(i + 1)
    
    avg_hit_rank = sum(hit_ranks) / len(hit_ranks) if hit_ranks else 0
    
    return {
        'total_patents': total_patents,
        'gold_distribution': gold_distribution,
        'pred_distribution': pred_distribution,
        'confusion_matrix': {'tp': tp, 'fp': fp, 'tn': tn, 'fn': fn},
        'binary_total': binary_total,
        'borderline_count': borderline_count,
        'metrics': metrics,
        'confidence_analysis': {
            'avg_hit_confidence': avg_hit_conf,
            'avg_miss_confidence': avg_miss_conf,
            'hit_count': len(hit_confidences),
            'miss_count': len(miss_confidences)
        },
        'ranking_analysis': {
            'total_hits': len(hit_ranks),
            'avg_hit_rank': avg_hit_rank,
            'hit_ranks': hit_ranks[:10]  # Top 10 for display
        }
    }

def generate_html_report(analysis_data):
    """Generate HTML report"""
    
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    html_content = f"""
<!DOCTYPE html>
<html lang="ja">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>æ³¨ç›®ç‰¹è¨±ä»•åˆ†ã‘ãã‚“ - æ€§èƒ½è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ</title>
    <style>
        body {{ font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; margin: 40px; background-color: #f5f5f5; }}
        .container {{ max-width: 1200px; margin: 0 auto; background: white; padding: 40px; border-radius: 10px; box-shadow: 0 0 20px rgba(0,0,0,0.1); }}
        h1 {{ color: #2c3e50; border-bottom: 3px solid #3498db; padding-bottom: 10px; }}
        h2 {{ color: #34495e; margin-top: 30px; }}
        .metric-grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 20px; margin: 20px 0; }}
        .metric-card {{ background: #ecf0f1; padding: 20px; border-radius: 8px; text-align: center; }}
        .metric-value {{ font-size: 2em; font-weight: bold; color: #2c3e50; }}
        .metric-label {{ color: #7f8c8d; margin-top: 5px; }}
        .status-good {{ color: #27ae60; }}
        .status-warning {{ color: #f39c12; }}
        .status-poor {{ color: #e74c3c; }}
        table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}
        th, td {{ padding: 12px; text-align: left; border-bottom: 1px solid #ddd; }}
        th {{ background-color: #3498db; color: white; }}
        .summary-box {{ background: #e8f4f8; padding: 20px; border-radius: 8px; margin: 20px 0; border-left: 5px solid #3498db; }}
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸ¯ æ³¨ç›®ç‰¹è¨±ä»•åˆ†ã‘ãã‚“ - æ€§èƒ½è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ</h1>
        
        <div class="summary-box">
            <h3>ğŸ“Š å®Ÿè¡Œæ¦‚è¦</h3>
            <p><strong>è©•ä¾¡æ—¥æ™‚:</strong> {timestamp}</p>
            <p><strong>ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿:</strong> {analysis_data['total_patents']}ä»¶ã®ç‰¹è¨±</p>
            <p><strong>ã‚·ã‚¹ãƒ†ãƒ æ§‹æˆ:</strong> GPT-4o-mini / LLM_confidenceãƒ™ãƒ¼ã‚¹ãƒ©ãƒ³ã‚­ãƒ³ã‚°</p>
            <p><strong>å¯¾è±¡ç™ºæ˜:</strong> æ¶²ä½“åˆ†é›¢è¨­å‚™ã®åŠ¹ç‡æ”¹å–„ã‚·ã‚¹ãƒ†ãƒ </p>
        </div>

        <h2>ğŸ¯ ç·åˆæ€§èƒ½æŒ‡æ¨™</h2>
        <div class="metric-grid">
            <div class="metric-card">
                <div class="metric-value {'status-good' if analysis_data['metrics']['accuracy'] >= 0.8 else 'status-warning' if analysis_data['metrics']['accuracy'] >= 0.7 else 'status-poor'}">
                    {analysis_data['metrics']['accuracy']:.1%}
                </div>
                <div class="metric-label">ç·åˆç²¾åº¦</div>
            </div>
            <div class="metric-card">
                <div class="metric-value {'status-good' if analysis_data['metrics']['precision'] >= 0.8 else 'status-warning' if analysis_data['metrics']['precision'] >= 0.7 else 'status-poor'}">
                    {analysis_data['metrics']['precision']:.1%}
                </div>
                <div class="metric-label">HITé©åˆç‡</div>
            </div>
            <div class="metric-card">
                <div class="metric-value {'status-good' if analysis_data['metrics']['recall'] >= 0.8 else 'status-warning' if analysis_data['metrics']['recall'] >= 0.7 else 'status-poor'}">
                    {analysis_data['metrics']['recall']:.1%}
                </div>
                <div class="metric-label">HITå†ç¾ç‡</div>
            </div>
            <div class="metric-card">
                <div class="metric-value {'status-good' if analysis_data['metrics']['f1'] >= 0.8 else 'status-warning' if analysis_data['metrics']['f1'] >= 0.7 else 'status-poor'}">
                    {analysis_data['metrics']['f1']:.1%}
                </div>
                <div class="metric-label">F1ã‚¹ã‚³ã‚¢</div>
            </div>
        </div>

        <h2>ğŸ“ˆ åˆ†é¡çµæœè©³ç´°</h2>
        <table>
            <tr><th>æŒ‡æ¨™</th><th>å€¤</th><th>è©•ä¾¡</th></tr>
            <tr><td>ç·ç‰¹è¨±æ•°</td><td>{analysis_data['total_patents']}</td><td>-</td></tr>
            <tr><td>äºŒå€¤åˆ†é¡å¯¾è±¡</td><td>{analysis_data['binary_total']}</td><td>-</td></tr>
            <tr><td>Borderlineã‚±ãƒ¼ã‚¹</td><td>{analysis_data['borderline_count']}</td><td>åˆ¥é€”åˆ†æ</td></tr>
            <tr><td>True Positive (æ­£è§£HIT)</td><td>{analysis_data['confusion_matrix']['tp']}</td><td>-</td></tr>
            <tr><td>False Positive (èª¤åˆ¤å®šHIT)</td><td>{analysis_data['confusion_matrix']['fp']}</td><td>-</td></tr>
            <tr><td>True Negative (æ­£è§£MISS)</td><td>{analysis_data['confusion_matrix']['tn']}</td><td>-</td></tr>
            <tr><td>False Negative (è¦‹é€ƒã—HIT)</td><td>{analysis_data['confusion_matrix']['fn']}</td><td>-</td></tr>
        </table>

        <h2>ğŸ² ä¿¡é ¼åº¦åˆ†æ</h2>
        <table>
            <tr><th>åˆ†é¡</th><th>å¹³å‡ä¿¡é ¼åº¦</th><th>ä»¶æ•°</th></tr>
            <tr><td>HITäºˆæ¸¬</td><td>{analysis_data['confidence_analysis']['avg_hit_confidence']:.3f}</td><td>{analysis_data['confidence_analysis']['hit_count']}</td></tr>
            <tr><td>MISSäºˆæ¸¬</td><td>{analysis_data['confidence_analysis']['avg_miss_confidence']:.3f}</td><td>{analysis_data['confidence_analysis']['miss_count']}</td></tr>
        </table>

        <h2>ğŸ“Š ãƒ©ãƒ³ã‚­ãƒ³ã‚°å“è³ª</h2>
        <div class="summary-box">
            <p><strong>HITç‰¹è¨±ã®å¹³å‡ãƒ©ãƒ³ã‚¯:</strong> {analysis_data['ranking_analysis']['avg_hit_rank']:.1f}ä½ (ç·{analysis_data['total_patents']}ä»¶ä¸­)</p>
            <p><strong>HITç‰¹è¨±ç·æ•°:</strong> {analysis_data['ranking_analysis']['total_hits']}ä»¶</p>
            <p><strong>ä¸Šä½HITç‰¹è¨±ãƒ©ãƒ³ã‚¯:</strong> {', '.join(map(str, analysis_data['ranking_analysis']['hit_ranks']))}</p>
        </div>

        <h2>âœ… æˆåŠŸåŸºæº–è©•ä¾¡</h2>
        <table>
            <tr><th>åŸºæº–</th><th>ç›®æ¨™</th><th>å®Ÿç¸¾</th><th>è©•ä¾¡</th></tr>
            <tr><td>ç·åˆç²¾åº¦</td><td>â‰¥80%</td><td>{analysis_data['metrics']['accuracy']:.1%}</td><td>{'âœ… é”æˆ' if analysis_data['metrics']['accuracy'] >= 0.8 else 'âŒ æœªé”æˆ'}</td></tr>
            <tr><td>HITæ¤œå‡ºç²¾åº¦</td><td>é«˜å†ç¾ç‡</td><td>{analysis_data['metrics']['recall']:.1%}</td><td>{'âœ… è‰¯å¥½' if analysis_data['metrics']['recall'] >= 0.8 else 'âš ï¸ è¦æ”¹å–„' if analysis_data['metrics']['recall'] >= 0.7 else 'âŒ ä¸ååˆ†'}</td></tr>
            <tr><td>ãƒ©ãƒ³ã‚­ãƒ³ã‚°å“è³ª</td><td>HITãŒä¸Šä½</td><td>å¹³å‡{analysis_data['ranking_analysis']['avg_hit_rank']:.1f}ä½</td><td>{'âœ… è‰¯å¥½' if analysis_data['ranking_analysis']['avg_hit_rank'] <= analysis_data['total_patents'] * 0.3 else 'âš ï¸ è¦æ”¹å–„'}</td></tr>
        </table>

        <h2>ğŸš€ æ”¹å–„ææ¡ˆ</h2>
        <div class="summary-box">
            <h4>å„ªå…ˆåº¦ï¼šé«˜</h4>
            <ul>
                {'<li>HITå†ç¾ç‡ã®å‘ä¸Š - False Negativeã‚’' + str(analysis_data['confusion_matrix']['fn']) + 'ä»¶ã‹ã‚‰å‰Šæ¸›</li>' if analysis_data['metrics']['recall'] < 0.8 else ''}
                {'<li>ç²¾åº¦å‘ä¸Š - False Positiveã‚’' + str(analysis_data['confusion_matrix']['fp']) + 'ä»¶ã‹ã‚‰å‰Šæ¸›</li>' if analysis_data['metrics']['precision'] < 0.8 else ''}
                {'<li>Borderlineã‚±ãƒ¼ã‚¹' + str(analysis_data['borderline_count']) + 'ä»¶ã®å‡¦ç†æ–¹é‡ç­–å®š</li>' if analysis_data['borderline_count'] > 0 else ''}
            </ul>
            
            <h4>å„ªå…ˆåº¦ï¼šä¸­</h4>
            <ul>
                <li>ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã®ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³èª¿æ•´</li>
                <li>ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æœ€é©åŒ–</li>
                <li>å‡¦ç†åŠ¹ç‡ã®å‘ä¸Š</li>
            </ul>
        </div>

        <footer style="margin-top: 40px; text-align: center; color: #7f8c8d; border-top: 1px solid #ddd; padding-top: 20px;">
            æ³¨ç›®ç‰¹è¨±ä»•åˆ†ã‘ãã‚“ - æ€§èƒ½è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ | Generated on {timestamp}
        </footer>
    </div>
</body>
</html>
"""
    
    return html_content

def generate_markdown_report(analysis_data):
    """Generate Markdown report"""
    
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    md_content = f"""# æ³¨ç›®ç‰¹è¨±ä»•åˆ†ã‘ãã‚“ - æ€§èƒ½è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ

## ğŸ“Š å®Ÿè¡Œæ¦‚è¦

- **è©•ä¾¡æ—¥æ™‚:** {timestamp}
- **ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿:** {analysis_data['total_patents']}ä»¶ã®ç‰¹è¨±
- **ã‚·ã‚¹ãƒ†ãƒ æ§‹æˆ:** GPT-4o-mini / LLM_confidenceãƒ™ãƒ¼ã‚¹ãƒ©ãƒ³ã‚­ãƒ³ã‚°
- **å¯¾è±¡ç™ºæ˜:** æ¶²ä½“åˆ†é›¢è¨­å‚™ã®åŠ¹ç‡æ”¹å–„ã‚·ã‚¹ãƒ†ãƒ 

## ğŸ¯ ç·åˆæ€§èƒ½æŒ‡æ¨™

| æŒ‡æ¨™ | å€¤ | è©•ä¾¡ |
|------|-----|------|
| ç·åˆç²¾åº¦ | {analysis_data['metrics']['accuracy']:.1%} | {'âœ… è‰¯å¥½' if analysis_data['metrics']['accuracy'] >= 0.8 else 'âš ï¸ è¦æ”¹å–„' if analysis_data['metrics']['accuracy'] >= 0.7 else 'âŒ ä¸ååˆ†'} |
| HITé©åˆç‡ | {analysis_data['metrics']['precision']:.1%} | {'âœ… è‰¯å¥½' if analysis_data['metrics']['precision'] >= 0.8 else 'âš ï¸ è¦æ”¹å–„' if analysis_data['metrics']['precision'] >= 0.7 else 'âŒ ä¸ååˆ†'} |
| HITå†ç¾ç‡ | {analysis_data['metrics']['recall']:.1%} | {'âœ… è‰¯å¥½' if analysis_data['metrics']['recall'] >= 0.8 else 'âš ï¸ è¦æ”¹å–„' if analysis_data['metrics']['recall'] >= 0.7 else 'âŒ ä¸ååˆ†'} |
| F1ã‚¹ã‚³ã‚¢ | {analysis_data['metrics']['f1']:.1%} | {'âœ… è‰¯å¥½' if analysis_data['metrics']['f1'] >= 0.8 else 'âš ï¸ è¦æ”¹å–„' if analysis_data['metrics']['f1'] >= 0.7 else 'âŒ ä¸ååˆ†'} |

## ğŸ“ˆ æ··åŒè¡Œåˆ—

```
                å®Ÿéš›
äºˆæ¸¬      HIT    MISS
HIT       {analysis_data['confusion_matrix']['tp']}     {analysis_data['confusion_matrix']['fp']}
MISS      {analysis_data['confusion_matrix']['fn']}     {analysis_data['confusion_matrix']['tn']}
```

- **True Positive (æ­£è§£HIT):** {analysis_data['confusion_matrix']['tp']}ä»¶
- **False Positive (èª¤åˆ¤å®šHIT):** {analysis_data['confusion_matrix']['fp']}ä»¶  
- **True Negative (æ­£è§£MISS):** {analysis_data['confusion_matrix']['tn']}ä»¶
- **False Negative (è¦‹é€ƒã—HIT):** {analysis_data['confusion_matrix']['fn']}ä»¶

## ğŸ² ä¿¡é ¼åº¦åˆ†æ

- **HITäºˆæ¸¬ã®å¹³å‡ä¿¡é ¼åº¦:** {analysis_data['confidence_analysis']['avg_hit_confidence']:.3f} ({analysis_data['confidence_analysis']['hit_count']}ä»¶)
- **MISSäºˆæ¸¬ã®å¹³å‡ä¿¡é ¼åº¦:** {analysis_data['confidence_analysis']['avg_miss_confidence']:.3f} ({analysis_data['confidence_analysis']['miss_count']}ä»¶)

## ğŸ“Š ãƒ©ãƒ³ã‚­ãƒ³ã‚°å“è³ª

- **HITç‰¹è¨±ã®å¹³å‡ãƒ©ãƒ³ã‚¯:** {analysis_data['ranking_analysis']['avg_hit_rank']:.1f}ä½ (ç·{analysis_data['total_patents']}ä»¶ä¸­)
- **HITç‰¹è¨±ç·æ•°:** {analysis_data['ranking_analysis']['total_hits']}ä»¶
- **ä¸Šä½HITç‰¹è¨±ãƒ©ãƒ³ã‚¯:** {', '.join(map(str, analysis_data['ranking_analysis']['hit_ranks']))}

## âœ… æˆåŠŸåŸºæº–è©•ä¾¡

| åŸºæº– | ç›®æ¨™ | å®Ÿç¸¾ | è©•ä¾¡ |
|------|------|------|------|
| ç·åˆç²¾åº¦ | â‰¥80% | {analysis_data['metrics']['accuracy']:.1%} | {'âœ… é”æˆ' if analysis_data['metrics']['accuracy'] >= 0.8 else 'âŒ æœªé”æˆ'} |
| HITæ¤œå‡ºç²¾åº¦ | é«˜å†ç¾ç‡ | {analysis_data['metrics']['recall']:.1%} | {'âœ… è‰¯å¥½' if analysis_data['metrics']['recall'] >= 0.8 else 'âš ï¸ è¦æ”¹å–„' if analysis_data['metrics']['recall'] >= 0.7 else 'âŒ ä¸ååˆ†'} |
| ãƒ©ãƒ³ã‚­ãƒ³ã‚°å“è³ª | HITãŒä¸Šä½ | å¹³å‡{analysis_data['ranking_analysis']['avg_hit_rank']:.1f}ä½ | {'âœ… è‰¯å¥½' if analysis_data['ranking_analysis']['avg_hit_rank'] <= analysis_data['total_patents'] * 0.3 else 'âš ï¸ è¦æ”¹å–„'} |

## ğŸš€ æ”¹å–„ææ¡ˆ

### å„ªå…ˆåº¦ï¼šé«˜

{'- HITå†ç¾ç‡ã®å‘ä¸Š - False Negativeã‚’' + str(analysis_data['confusion_matrix']['fn']) + 'ä»¶ã‹ã‚‰å‰Šæ¸›' if analysis_data['metrics']['recall'] < 0.8 else ''}
{'- ç²¾åº¦å‘ä¸Š - False Positiveã‚’' + str(analysis_data['confusion_matrix']['fp']) + 'ä»¶ã‹ã‚‰å‰Šæ¸›' if analysis_data['metrics']['precision'] < 0.8 else ''}
{'- Borderlineã‚±ãƒ¼ã‚¹' + str(analysis_data['borderline_count']) + 'ä»¶ã®å‡¦ç†æ–¹é‡ç­–å®š' if analysis_data['borderline_count'] > 0 else ''}

### å„ªå…ˆåº¦ï¼šä¸­

- ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã®ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³èª¿æ•´
- ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æœ€é©åŒ–  
- å‡¦ç†åŠ¹ç‡ã®å‘ä¸Š

## ğŸ“‹ æŠ€è¡“çš„è©³ç´°

- **äºŒå€¤åˆ†é¡å¯¾è±¡:** {analysis_data['binary_total']}ä»¶ (Borderline {analysis_data['borderline_count']}ä»¶ã¯é™¤å¤–)
- **ã‚´ãƒ¼ãƒ«ãƒ‰æ¨™æº–åˆ†å¸ƒ:** HIT {analysis_data['gold_distribution']['hit']}ä»¶, MISS {analysis_data['gold_distribution']['miss']}ä»¶, Borderline {analysis_data['gold_distribution']['borderline']}ä»¶
- **äºˆæ¸¬åˆ†å¸ƒ:** HIT {analysis_data['pred_distribution']['hit']}ä»¶, MISS {analysis_data['pred_distribution']['miss']}ä»¶

---
*Generated on {timestamp} - æ³¨ç›®ç‰¹è¨±ä»•åˆ†ã‘ãã‚“ Performance Evaluation*
"""
    
    return md_content

def main():
    """Main execution function"""
    
    print("=== æ³¨ç›®ç‰¹è¨±ä»•åˆ†ã‘ãã‚“ æ€§èƒ½è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ ===")
    
    # Analyze data
    print("1. ãƒ‡ãƒ¼ã‚¿åˆ†æå®Ÿè¡Œä¸­...")
    analysis_data = load_and_analyze()
    
    # Generate reports
    print("2. HTMLãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¸­...")
    html_content = generate_html_report(analysis_data)
    
    print("3. Markdownãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆä¸­...")
    md_content = generate_markdown_report(analysis_data)
    
    # Save reports
    reports_dir = Path("archive/reports")
    reports_dir.mkdir(parents=True, exist_ok=True)
    
    timestamp_str = datetime.now().strftime("%Y%m%d_%H%M")
    
    html_path = reports_dir / f"patent_screening_evaluation_report_{timestamp_str}.html"
    md_path = reports_dir / f"patent_screening_evaluation_report_{timestamp_str}.md"
    
    with open(html_path, 'w', encoding='utf-8') as f:
        f.write(html_content)
        
    with open(md_path, 'w', encoding='utf-8') as f:
        f.write(md_content)
    
    print(f"4. ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†!")
    print(f"   HTML: {html_path}")
    print(f"   Markdown: {md_path}")
    
    # Summary
    print(f"\n=== è©•ä¾¡ã‚µãƒãƒªãƒ¼ ===")
    print(f"ç·åˆç²¾åº¦: {analysis_data['metrics']['accuracy']:.1%}")
    print(f"HITé©åˆç‡: {analysis_data['metrics']['precision']:.1%}")  
    print(f"HITå†ç¾ç‡: {analysis_data['metrics']['recall']:.1%}")
    print(f"F1ã‚¹ã‚³ã‚¢: {analysis_data['metrics']['f1']:.1%}")
    
    success_criteria_met = (
        analysis_data['metrics']['accuracy'] >= 0.8 and
        analysis_data['metrics']['recall'] >= 0.8 and
        analysis_data['ranking_analysis']['avg_hit_rank'] <= analysis_data['total_patents'] * 0.3
    )
    
    print(f"\næˆåŠŸåŸºæº–é”æˆ: {'âœ… YES' if success_criteria_met else 'âŒ NO'}")
    
    return {
        'html_path': str(html_path),
        'md_path': str(md_path),
        'metrics': analysis_data['metrics'],
        'success_criteria_met': success_criteria_met
    }

if __name__ == "__main__":
    result = main()